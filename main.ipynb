{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Jupyter Notebooks for implementing the German Traffic Sign Classification CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Gibt an, ob wir auf einer GPU oder CPU trainieren \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.3433, 0.3151, 0.3253])\n",
      "Std: tensor([0.1690, 0.1713, 0.1815])\n"
     ]
    }
   ],
   "source": [
    "# Hier wird unser mean und unser std f√ºr die Normalisierung der Bilder berechnet\n",
    "# dataset = ImageFolder(\"GTSRB\\Final_Training\\Images\", transform=transforms.ToTensor())\n",
    "dataset = ImageFolder(\"GTSRB/Final_Training/Images\", transform=transforms.ToTensor())\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform definieren zum Einlesen der Test- und Trainingsdaten\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.1723866..3.9978275].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: einmalige_vorfahrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1873635e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Einlesen der Daten und anschlie√üende Ausgabe eines Bildes inkl. Label zur √ºberpr√ºfung\n",
    "train_dataset = ImageFolder(root='GTSRB/Final_Training/Images', transform=transform)\n",
    "\n",
    "test_dataset = ImageFolder(root='GTSRB/Final_Test/Images', transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True)\n",
    "num_classes = 43\n",
    "# --------- Testing ---------\n",
    "img, label = test_dataset[4000]\n",
    "label_string = test_dataset.classes[label]\n",
    "print(\"Label:\", label_string)\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "#print(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren unseres CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 512) \n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.3)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.3)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderes CNN definieren zum Testen\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 750) \n",
    "        self.bn5 = nn.BatchNorm1d(750)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(750, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(self.bn3(x))\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the model and definition of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierung des Modells\n",
    "model = CNN(num_classes).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initiale Lernrate und Optimizer\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# ReduceLROnPlateau Scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the model and definition of hyperparameters for the Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(lr, weight_decay, threshold):\n",
    "    # Initialisierung des Modells\n",
    "    model = CNN(num_classes).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initiale Lernrate und Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ReduceLROnPlateau Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=threshold, verbose=True)\n",
    "    return model, loss_func, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'model/new_cnn_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with accuracy plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Activation of the Model Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function to capture the outputs\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN(num_classes=43)\n",
    "model.to(device)\n",
    "# Initiale Lernrate und Optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Register hooks to capture activations from convolutional and fully connected layers\n",
    "model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "\n",
    "# Register hooks for ReLU activations\n",
    "model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "\n",
    "# Register hooks for fully connected layers\n",
    "model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "\n",
    "# Register hooks for ReLU activations in fully connected layers\n",
    "model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "model.relu8.register_forward_hook(get_activation('relu8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model with Plot of the Accuracy each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for the current epoch\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_func(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Calculate average loss for the current epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Accuracy on training and test data\n",
    "    train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    #current_lr =  scheduler.get_last_lr()\n",
    "    #print(f\"Learning Rate: {current_lr}\")\n",
    "    \n",
    "    \n",
    "    # Step the scheduler to adjust learning rate based on validation loss (or training loss)\n",
    "    #scheduler.step(avg_loss)\n",
    "\n",
    "    # Clear the current plot and update with new data\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the updated plot\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# Save the model's weights\n",
    "torch.save(model.state_dict(), 'model/new_cnn_architecture.pth')\n",
    "\n",
    "# Show the final plot after all epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_train(lr, weight_decay, threshold):\n",
    "    # Training starten (Stellen Sie sicher, dass die Funktion training_loop richtig definiert ist)\n",
    "    model, loss_func, optimizer, scheduler = training_loop(lr, weight_decay, threshold)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"Start training the model with:\\n lr: {lr} \\n weight_decay: {weight_decay} \\n threshold: {threshold}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    num_epochs = 80\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "\n",
    "    # Initialisiere das Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Ordner f√ºr gespeicherte Modelle und Bilder erstellen, falls nicht vorhanden\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0  # Verfolge den Verlust f√ºr die aktuelle Epoche\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # L√∂sche die vorherigen Gradienten\n",
    "            outputs = model(images)  # Vorw√§rtsdurchlauf\n",
    "            loss = loss_func(outputs, labels)  # Verlust berechnen\n",
    "            loss.backward()  # R√ºckw√§rtsdurchlauf\n",
    "            optimizer.step()  # Gewicht anpassen\n",
    "            \n",
    "            running_loss += loss.item()  # Verlust akkumulieren\n",
    "\n",
    "        # Durchschnittlichen Verlust f√ºr die aktuelle Epoche berechnen\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Genauigkeit auf Trainings- und Testdaten berechnen\n",
    "        train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "        test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "        \n",
    "        # Den Scheduler nach dem Training der aktuellen Epoche anpassen\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Plot mit den aktuellen Daten aktualisieren\n",
    "        plt.clf()  # Figur l√∂schen\n",
    "        plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'LR: {lr}, WD: {weight_decay}, Th: {threshold} - Training and Test Accuracy per Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if(epoch == num_epochs -1):\n",
    "            # Speicher den Plot im 'img' Ordner mit dem entsprechenden Dateinamen\n",
    "            plot_filename = f'img/plot_lr_{lr}_wd_{weight_decay}_th_{threshold}.png'\n",
    "            plt.savefig(plot_filename)\n",
    "        \n",
    "        # Zeige den aktualisierten Plot\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    # Speicher das Modell im 'model' Ordner mit dem entsprechenden Dateinamen\n",
    "    model_filename = f'model/model_lr_{lr}_wd_{weight_decay}_th_{threshold}.pth'\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "    # Zeige das finale Plot nach allen Epochen\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Hyperparameter-Werte\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "weight_decays = [0.0001, 0.0007, 0.001, 0.005]\n",
    "thresholds = [0.1, 0.01, 0.001]\n",
    "\n",
    "# Schleife √ºber alle Kombinationen\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for th in thresholds:\n",
    "            # Rufe die Mode_train Funktion mit den aktuellen Hyperparametern auf\n",
    "            mode_train(lr, wd, th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code from yt \n",
    "(https://www.youtube.com/watch?v=ZBfpkepdZlw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\t\t# load in the data in batches\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # backward propagation and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # at end of epoch check validation loss and acc\n",
    "    with torch.no_grad():\n",
    "      \t# switch model to eval (not train) model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_loss = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total += labels.size(0)\n",
    "            # calculate predictions\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # calculate actual values\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # calculate the loss\n",
    "            all_val_loss.append(loss_func(outputs, labels).item())\n",
    "        # calculate val-loss\n",
    "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
    "        # calculate val-accuracy\n",
    "        mean_val_acc = 100 * (correct / total)\n",
    "    print(\n",
    "        'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(\n",
    "            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")\n",
    "\n",
    "#torch.save(model.state_dict(), \"model/good_models/99_95_and_95_77/training_99_95_test_95_77.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse, ohne die Anzahl der Klassen als Eingabe zu ben√∂tigen\n",
    "def calculate_class_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Sammle die Richtigkeit der Vorhersagen f√ºr jede Klasse\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    # Berechne die Accuracy f√ºr jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names\n",
    "\n",
    "# Berechne und plotte die Klassenaccuracy nach den 40 Trainingsepochen\n",
    "class_accuracies, class_names = calculate_class_accuracy(model, test_loader, device)\n",
    "\n",
    "# Plot Histogramm der Klassenaccuracies mit Klassennamen auf der x-Achse\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(class_names)), class_accuracies, color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy per Class after Training')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)  # Setzt die Klassennamen als x-Achsen-Beschriftungen\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse und speichert falsche Vorhersagen\n",
    "def calculate_class_accuracy_and_misclassifications(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    misclassified_counts = defaultdict(lambda: [0] * num_classes)  # Z√§hlt Fehlklassifizierungen je Klasse\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Z√§hlt korrekte und fehlerhafte Vorhersagen\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "                else:\n",
    "                    misclassified_counts[label][predicted[i].item()] += 1  # Fehlklassifizierung speichern\n",
    "    \n",
    "    # Berechne die Accuracy f√ºr jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names, misclassified_counts\n",
    "\n",
    "# Berechne Accuracy und Fehlklassifizierungen\n",
    "class_accuracies, class_names, misclassified_counts = calculate_class_accuracy_and_misclassifications(model, test_loader, device)\n",
    "\n",
    "# Erstelle Plots f√ºr Klassen mit einer Accuracy < 80%\n",
    "for i, accuracy in enumerate(class_accuracies):\n",
    "    if accuracy < 90:\n",
    "        # Bereite Daten f√ºr die Fehlklassifizierungen dieser Klasse auf\n",
    "        misclassified_counts_for_class = misclassified_counts[i]\n",
    "        misclassified_class_names = [class_names[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        misclassified_class_counts = [misclassified_counts_for_class[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        \n",
    "        # Plot der Fehlklassifizierungen f√ºr die Klasse\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(misclassified_class_names, misclassified_class_counts, color='salmon')\n",
    "        plt.xlabel('Predicted Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f\"Misclassifications for Class '{class_names[i]}' (Accuracy: {accuracy:.2f}%)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to test an old model, you can do this right here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN Model Class vor Evaluation. Make sure you replace this with the correct Conv Layers and FC Layers of your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    " \n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    " \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    " \n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    " \n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    " \n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    " \n",
    " \n",
    "        self.fc6 = nn.Linear(128, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    " \n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    " \n",
    " \n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    " \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    " \n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    " \n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    " \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    " \n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    " \n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    " \n",
    "        out = self.fc9(out)\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide now informations such as path and the number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'model/model_with_acitvations.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Das Modell unter '{model_path}' wurde nicht gefunden.\")\n",
    " \n",
    "# Modell laden\n",
    "trained_model = Net(num_classes=43)\n",
    "trained_model.to(device)\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 99.95%\n",
      "Accuracy on test data: 95.77%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "trained_model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(trained_model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(trained_model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Explainable Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Select a random or specific image by changing the index\n",
    "rand = 9  # √Ñndere die Zahl hier, um ein anderes Bild anzuzeigen\n",
    "image = images[rand:rand+1]\n",
    "label = labels[rand]\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "# Display the input image\n",
    "plt.imshow(image[0].permute(1, 2, 0).cpu())  # Bild vom Tensor in NumPy-Format umwandeln\n",
    "plt.title(f'Input Image - Label: {label.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the activations dictionary\n",
    "activations = {}\n",
    "# Ensure hooks are registered (they should be from earlier)\n",
    "# Run the model on the sample image\n",
    "output = model(image)\n",
    "print(activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aktivierungen abrufen\n",
    "act = activations['conv_layer1']\n",
    "print(f\"Shape of conv1 activations: {act.shape}\")\n",
    "\n",
    "# Filter-Anzahl\n",
    "num_filters = act.shape[1]\n",
    "\n",
    "# Dynamische Subplots\n",
    "rows = math.ceil(num_filters / 5)\n",
    "fig, axes = plt.subplots(rows, 5, figsize=(15, rows * 3))\n",
    "\n",
    "for idx in range(num_filters):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(act[0, idx].detach().cpu(), cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "# Unbenutzte Subplots entfernen (falls n√∂tig)\n",
    "for idx in range(num_filters, rows * 5):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Maximazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_maximization(model, layer_name, filter_index, input_size=(1, 3, 35, 35), lr=0.1, iterations=30):\n",
    "    device = next(model.parameters()).device  # Ger√§t des Modells (CPU oder GPU)\n",
    "    \n",
    "    # Initialisierung der Eingabe\n",
    "    input_image = torch.randn(input_size, requires_grad=True, device=device)  # Auf das richtige Ger√§t legen\n",
    "    \n",
    "    optimizer = optim.Adam([input_image], lr=lr, weight_decay=1e-6)\n",
    "    activations = {}\n",
    "\n",
    "    # Hook-Funktion f√ºr die gew√ºnschte Schicht\n",
    "    def hook_function(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Hook registrieren\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_function)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)  # Modell auf dem richtigen Ger√§t ausf√ºhren\n",
    "        act = activations[layer_name][0, filter_index]\n",
    "        loss = -torch.mean(act)  # Aktivierung maximieren\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Eingabewerte beschr√§nken\n",
    "        input_image.data = torch.clamp(input_image.data, 0, 1)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Iteration {i+1}/{iterations}, Loss: {-loss.item():.4f}\")\n",
    "\n",
    "    hook.remove()\n",
    "    return input_image.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer1num_filters_relu1 = model.conv_layer1.out_channels\n",
    "am_images_relu1 = []\n",
    "\n",
    "for filter_idx in range(conv_layer1num_filters_relu1):\n",
    "    print(f\"\\nGenerating image for relu1 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(model, 'relu1', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30)\n",
    "    am_images_relu1.append(am_image)\n",
    "\n",
    "print(\"\\nCompleted activation maximization for relu1.\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu1[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild darstellen\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu1 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu 2\n",
    "\n",
    "# Anzahl der Filter in der zweiten Schicht (ReLU2)\n",
    "conv_layer2num_filters_relu2 = model.conv_layer2.out_channels\n",
    "am_images_relu2 = []\n",
    "\n",
    "# Aktivierungsmaximierung f√ºr alle Filter der ReLU2-Schicht\n",
    "for filter_idx in range(conv_layer2num_filters_relu2):\n",
    "    print(f\"\\nGenerating image for relu2 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(\n",
    "        model, 'relu2', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30\n",
    "    )\n",
    "    am_images_relu2.append(am_image)\n",
    "\n",
    "print(\"\\nCompleted activation maximization for relu2.\")\n",
    "\n",
    "# Darstellung der Ergebnisse\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # Layout f√ºr 10 Filter\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu2[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu2 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dummy-Eingabebild erstellen (RGB-Bild, normalisiert zwischen 0 und 1) und auf das Ger√§t verschieben\n",
    "img, label = test_dataset[4000]  # Zuf√§lliges Bild (Channels, Height, Width)\n",
    "input_image = img.to(device)  # Die Variable `img` sollte `input_image` zugewiesen werden.\n",
    "\n",
    "# Weiterleitung durch die Schichten bis ReLU2\n",
    "with torch.no_grad():\n",
    "    out_conv1 = model.relu1(model.conv_layer1(input_image))  # Erste Schicht + ReLU\n",
    "    out_pool1 = model.max_pool1(out_conv1)                  # Max-Pooling\n",
    "    out_conv2 = model.relu2(model.conv_layer2(out_pool1))   # Zweite Schicht + ReLU\n",
    "\n",
    "# Eingabebild visualisieren\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img.permute(1, 2, 0).cpu())  # (C, H, W) -> (H, W, C) und auf CPU f√ºr die Darstellung\n",
    "plt.axis('off')\n",
    "plt.title(\"Eingabebild\")\n",
    "plt.show()\n",
    "\n",
    "# Ausgabe-Shape der Aktivierung\n",
    "print(\"Shape der Aktivierung nach ReLU2:\", out_conv2.shape)\n",
    "\n",
    "# Aktivierungskarten der ersten 10 Filter visualisieren\n",
    "num_filters = min(10, out_conv2.shape[1])  # Zeige maximal 10 Filter\n",
    "fig, axes = plt.subplots(1, num_filters, figsize=(15, 6))\n",
    "\n",
    "for i in range(num_filters):\n",
    "    activation_map = out_conv2[0, i].cpu().numpy()  # Aktivierung f√ºr Filter i\n",
    "    \n",
    "    # Falls die Aktivierungskarten 1D sind, reshape sie zu 2D (H, W)\n",
    "    if activation_map.ndim == 1:\n",
    "        activation_map = activation_map.reshape(35, 35)  # Beispiel f√ºr die Gr√∂√üe 35x35 (oder was du ben√∂tigst)\n",
    "    \n",
    "    axes[i].imshow(activation_map, cmap='viridis')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle(\"Aktivierungskarten f√ºr ReLU2 (bestimmtes Bild)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "def grad_cam(model, input_image, target_class, layer_name):\n",
    "    \"\"\"\n",
    "    Implement Grad-CAM for a specific layer and target class.\n",
    "    Args:\n",
    "        model: Trained CNN model.\n",
    "        input_image: Preprocessed input image (Tensor).\n",
    "        target_class: Index of the target class for Grad-CAM.\n",
    "        layer_name: Name of the convolutional layer for Grad-CAM.\n",
    "    Returns:\n",
    "        heatmap: Grad-CAM heatmap for the target class.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Dictionary to hold gradients and activations\n",
    "    gradients = {}\n",
    "    activations = {}\n",
    "\n",
    "    # Hook to capture gradients\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients[layer_name] = grad_output[0]\n",
    "\n",
    "    # Hook to capture activations\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Register hooks on the desired layer\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    input_image = input_image.unsqueeze(0)  # Add batch dimension\n",
    "    input_image = input_image.to(next(model.parameters()).device)  # Move to device\n",
    "    output = model(input_image)\n",
    "\n",
    "    # Backward pass for the target class\n",
    "    model.zero_grad()\n",
    "    target_score = output[0, target_class]\n",
    "    target_score.backward()\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    grads = gradients[layer_name]  # Gradients from backward pass\n",
    "    acts = activations[layer_name]  # Activations from forward pass\n",
    "    pooled_grads = torch.mean(grads, dim=(2, 3))  # Global average pooling\n",
    "\n",
    "    # Weight activations by pooled gradients\n",
    "    acts = acts * pooled_grads.view(1, -1, 1, 1)  # Apply broadcasting\n",
    "\n",
    "    # Average across the channels\n",
    "    heatmap = torch.mean(acts, dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def show_grad_cam(image, heatmap, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay Grad-CAM heatmap on the input image.\n",
    "    Args:\n",
    "        image: Original input image (as a numpy array).\n",
    "        heatmap: Grad-CAM heatmap.\n",
    "        alpha: Transparency for the overlay.\n",
    "    \"\"\"\n",
    "    # Normalize image to [0, 1]\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    # Resize heatmap to match image dimensions using PyTorch\n",
    "    heatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    heatmap_resized = F.interpolate(heatmap_tensor, size=image.shape[:2], mode='bilinear', align_corners=False)\n",
    "    heatmap_resized = heatmap_resized.squeeze().numpy()  # Remove added dims\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)  # Scale heatmap to [0, 255]\n",
    "\n",
    "    # Convert heatmap to RGB\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]  # Convert heatmap to RGB\n",
    "    heatmap_colored = heatmap_colored / np.max(heatmap_colored)  # Normalize heatmap\n",
    "\n",
    "    # Combine heatmap with original image\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image\n",
    "    overlay = overlay / np.max(overlay)  # Normalize overlay to [0, 1]\n",
    "\n",
    "    # Plot the result\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Beispielanwendung\n",
    "image, label = next(iter(train_loader))  # Nimm ein Bild aus dem Testset\n",
    "\n",
    "image = image[0]  # Nur ein Bild\n",
    "label = label[0]\n",
    "\n",
    "original_image = image.permute(1, 2, 0).numpy()  # Konvertiere f√ºr Visualisierung\n",
    "\n",
    "# Berechne Grad-CAM f√ºr eine Schicht\n",
    "layer_name = \"conv_layer5\"  # Letzte Convolutional-Schicht\n",
    "target_class = label  # Beispielzielklasse\n",
    "heatmap = grad_cam(trained_model, image, target_class, layer_name)\n",
    "\n",
    "# Visualisiere das Ergebnis\n",
    "show_grad_cam(original_image, heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape before processing: torch.Size([3, 35, 35])\n",
      "0\n",
      "100_kmh\n",
      "100_kmh\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Beispielanwendung\n",
    "dataset = train_loader.dataset\n",
    "\n",
    "# Hol dir ein bestimmtes Bild und Label mit Index\n",
    "index = 5  # Beispiel: Nimm das Bild mit Index 42\n",
    "image, label = dataset[index]\n",
    "\n",
    "# Pr√ºfe die Form des Bildes\n",
    "print(f\"Image shape before processing: {image.shape}\")\n",
    "\n",
    "# Falls das Bild nur zwei Dimensionen hat, f√ºge eine Kanalachse hinzu\n",
    "if len(image.shape) == 2:  # Graustufenbild\n",
    "    image = image.unsqueeze(0)  # F√ºge Kanalachse hinzu\n",
    "\n",
    "# Visualisierung vorbereiten\n",
    "original_image = image.permute(1, 2, 0).numpy()  # Konvertiere f√ºr Visualisierung\n",
    "\n",
    "# Berechne Grad-CAM f√ºr eine Schicht\n",
    "layer_name = \"conv_layer5\"  # Letzte Convolutional-Schicht\n",
    "target_class = 0  # Nutze das Label des Bildes als Zielklasse\n",
    "heatmap = grad_cam(trained_model, image, target_class, layer_name)\n",
    "\n",
    "print(label)\n",
    "print(train_dataset.classes[label])\n",
    "print(train_dataset.classes[target_class])\n",
    "# Visualisiere das Ergebnis\n",
    "show_grad_cam(original_image, heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
