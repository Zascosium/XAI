{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Jupyter Notebooks for implementing the German Traffic Sign Classification CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Gibt an, ob wir auf einer GPU oder CPU trainieren \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.3403, 0.3122, 0.3215])\n",
      "Std: tensor([0.1604, 0.1600, 0.1694])\n"
     ]
    }
   ],
   "source": [
    "# Hier wird unser mean und unser std für die Normalisierung der Bilder berechnet\n",
    "# dataset = ImageFolder(\"GTSRB\\Final_Training\\Images\", transform=transforms.ToTensor())\n",
    "transform_mean_std = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = ImageFolder(\"C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/train_images\", transform=transform_mean_std)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform definieren zum Einlesen der Test- und Trainingsdaten\n",
    "img_size = 35 \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist()) \n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(20),  # Zufällige Rotation um ±20 Grad\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),  # Zufälliges Zuschneiden und Skalieren\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Helligkeit und Farbvariation\n",
    "    transforms.RandomHorizontalFlip(),  # Zufälliges horizontales Spiegeln\n",
    "    transforms.ToTensor(),  # Umwandlung zu einem Tensor\n",
    "    transforms.Normalize(mean.tolist(), std.tolist())  # Normalisierung\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.3422389..4.2980943].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 30_kmh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x198749940a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Einlesen der Daten und anschließende Ausgabe eines Bildes inkl. Label zur überprüfung\n",
    "train_dataset = ImageFolder(root='C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/train_images', transform=transform)\n",
    "train_transforms_dataset = ImageFolder(root='C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/train_images', transform=train_transforms)\n",
    "\n",
    "test_dataset = ImageFolder(root='C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/test_images', transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=True)\n",
    "train_transforms_loader = DataLoader(train_transforms_dataset, batch_size= batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True)\n",
    "num_classes = 43\n",
    "# --------- Testing ---------\n",
    "img, label = train_dataset[4000]\n",
    "label_string = test_dataset.classes[label]\n",
    "print(\"Label:\", label_string)\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "#print(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Dynamische Berechnung der FC-Eingabegröße\n",
    "        self.flatten = nn.Flatten()\n",
    "        # dummy_input = torch.zeros(batch_size, 3, image_size, image_size)  # Dummy-Eingabe mit typischer Größe (z.B. 224x224)\n",
    "        # fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(128, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "        \"\"\"Hilfsfunktion, um die Eingabegröße für die Fully Connected Layers zu berechnen.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)  # Flatten the output\n",
    "        return x.size(1)  # Gib die Anzahl der Features zurück\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)  # Flatten\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderes CNN definieren zum Testen\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 750) \n",
    "        self.bn5 = nn.BatchNorm1d(750)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(750, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(self.bn3(x))\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the model and definition of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierung des Modells\n",
    "model = CNN(num_classes).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initiale Lernrate und Optimizer\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# ReduceLROnPlateau Scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'model/new_cnn_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with accuracy plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Activation of the Model Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x19876bb73a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function to capture the outputs\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN(num_classes=43)\n",
    "model.to(device)\n",
    "# Initiale Lernrate und Optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Register hooks to capture activations from convolutional and fully connected layers\n",
    "model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "\n",
    "# Register hooks for ReLU activations\n",
    "model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "\n",
    "# Register hooks for fully connected layers\n",
    "model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "\n",
    "# Register hooks for ReLU activations in fully connected layers\n",
    "model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "model.relu8.register_forward_hook(get_activation('relu8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Visualization while Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Loss: nan\n",
      "Training Accuracy: 3.67%\n",
      "Test Accuracy: 3.82%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     70\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     outputs_2d \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m pca_results\u001b[38;5;241m.\u001b[39mappend(outputs_2d)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Visualisierungen aktualisieren\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:511\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\v814u63\\AppData\\Local\\anaconda3\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Zusätzliche Listen für neue Visualisierungen\n",
    "last_layer_weights = []\n",
    "pca_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "# Initialize the plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Variable für Loader-Wechsel\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None  # Speichert die Epoche des Loaderswechsels\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for the current epoch\n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_func(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Calculate average loss for the current epoch\n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "    # Accuracy on training and test data\n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Check if loss has increased and Loader hasn't switched yet\n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader  # Wechsel zu transformiertem Loader\n",
    "        switch_epoch = epoch  # Speichere die Epoche des Wechsels\n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    # Update the previous loss\n",
    "    previous_loss = avg_loss\n",
    "\n",
    "    # Visualisierung der Gewichte der letzten Schicht (vor dem Output Layer)\n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy()  # Anpassen, falls die Schicht anders heißt\n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    # PCA auf die Output-Vektoren anwenden\n",
    "    with torch.no_grad():\n",
    "        pca = PCA(n_components=2)\n",
    "        outputs_2d = pca.fit_transform(outputs.cpu().numpy())\n",
    "    pca_results.append(outputs_2d)\n",
    "\n",
    "    # Visualisierungen aktualisieren\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: PCA der Outputs\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('PCA of Output Vectors')\n",
    "    plt.xlabel('PCA Dimension 1')\n",
    "    plt.ylabel('PCA Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualisierung der letzten Layer-Gewichte\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "            print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "            plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "            break\n",
    "\n",
    "    # Display the updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# Save the model's weights\n",
    "torch.save(model.state_dict(), f'model/gute_modell.pth')\n",
    "torch.save(model.state_dict(), f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "# Show the final plot after all epochs\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Zusätzliche Listen für neue Visualisierungen\n",
    "last_layer_weights = []\n",
    "tsne_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "# Initialize the plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Variable für Loader-Wechsel\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None  # Speichert die Epoche des Loaderswechsels\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for the current epoch\n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_func(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Calculate average loss for the current epoch\n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "    # Accuracy on training and test data\n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Check if loss has increased and Loader hasn't switched yet\n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader  # Wechsel zu transformiertem Loader\n",
    "        switch_epoch = epoch  # Speichere die Epoche des Wechsels\n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    # Update the previous loss\n",
    "    previous_loss = avg_loss\n",
    "\n",
    "    # Visualisierung der Gewichte der letzten Schicht (vor dem Output Layer)\n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy()  # Anpassen, falls die Schicht anders heißt\n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    # t-SNE auf die Output-Vektoren anwenden\n",
    "    with torch.no_grad():\n",
    "        tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "        outputs_2d = tsne.fit_transform(outputs.cpu().numpy())\n",
    "    tsne_results.append(outputs_2d)\n",
    "\n",
    "    # Visualisierungen aktualisieren\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: t-SNE der Outputs\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('t-SNE of Output Vectors')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualisierung der letzten Layer-Gewichte\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "        print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "        plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "        break\n",
    "\n",
    "    # Display the updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# Save the model's weights\n",
    "torch.save(model.state_dict(), f'model/gute_modell.pth')\n",
    "torch.save(model.state_dict(), f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "# Show the final plot after all epochs\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code from yt \n",
    "(https://www.youtube.com/watch?v=ZBfpkepdZlw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\t\t# load in the data in batches\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # backward propagation and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # at end of epoch check validation loss and acc\n",
    "    with torch.no_grad():\n",
    "      \t# switch model to eval (not train) model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_loss = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total += labels.size(0)\n",
    "            # calculate predictions\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # calculate actual values\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # calculate the loss\n",
    "            all_val_loss.append(loss_func(outputs, labels).item())\n",
    "        # calculate val-loss\n",
    "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
    "        # calculate val-accuracy\n",
    "        mean_val_acc = 100 * (correct / total)\n",
    "    print(\n",
    "        'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(\n",
    "            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")\n",
    "\n",
    "#torch.save(model.state_dict(), \"model/good_models/99_95_and_95_77/training_99_95_test_95_77.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse, ohne die Anzahl der Klassen als Eingabe zu benötigen\n",
    "def calculate_class_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Sammle die Richtigkeit der Vorhersagen für jede Klasse\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    # Berechne die Accuracy für jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names\n",
    "\n",
    "# Berechne und plotte die Klassenaccuracy nach den 40 Trainingsepochen\n",
    "class_accuracies, class_names = calculate_class_accuracy(model, test_loader, device)\n",
    "\n",
    "# Plot Histogramm der Klassenaccuracies mit Klassennamen auf der x-Achse\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(class_names)), class_accuracies, color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy per Class after Training')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)  # Setzt die Klassennamen als x-Achsen-Beschriftungen\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class and save wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse und speichert falsche Vorhersagen\n",
    "def calculate_class_accuracy_and_misclassifications(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    misclassified_counts = defaultdict(lambda: [0] * num_classes)  # Zählt Fehlklassifizierungen je Klasse\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Zählt korrekte und fehlerhafte Vorhersagen\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "                else:\n",
    "                    misclassified_counts[label][predicted[i].item()] += 1  # Fehlklassifizierung speichern\n",
    "    \n",
    "    # Berechne die Accuracy für jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names, misclassified_counts\n",
    "\n",
    "# Berechne Accuracy und Fehlklassifizierungen\n",
    "class_accuracies, class_names, misclassified_counts = calculate_class_accuracy_and_misclassifications(model, test_loader, device)\n",
    "\n",
    "# Erstelle Plots für Klassen mit einer Accuracy < 80%\n",
    "for i, accuracy in enumerate(class_accuracies):\n",
    "    if accuracy < 90:\n",
    "        # Bereite Daten für die Fehlklassifizierungen dieser Klasse auf\n",
    "        misclassified_counts_for_class = misclassified_counts[i]\n",
    "        misclassified_class_names = [class_names[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        misclassified_class_counts = [misclassified_counts_for_class[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        \n",
    "        # Plot der Fehlklassifizierungen für die Klasse\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(misclassified_class_names, misclassified_class_counts, color='salmon')\n",
    "        plt.xlabel('Predicted Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f\"Misclassifications for Class '{class_names[i]}' (Accuracy: {accuracy:.2f}%)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to test an old model, you can do this right here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN Model Class vor Evaluation. Make sure you replace this with the correct Conv Layers and FC Layers of your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, image_size = 100):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Dynamische Berechnung der FC-Eingabegröße\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(batch_size, 3, image_size, image_size)  # Dummy-Eingabe mit typischer Größe (z.B. 224x224)\n",
    "        fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(fc_input_size, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "        \"\"\"Hilfsfunktion, um die Eingabegröße für die Fully Connected Layers zu berechnen.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)  # Flatten the output\n",
    "        return x.size(1)  # Gib die Anzahl der Features zurück\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)  # Flatten\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide now informations such as path and the number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model\\optim_loop\\SGD\\image_size_100_train_96.64362773852942_test_96.6159265316392.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Das Modell unter '{model_path}' wurde nicht gefunden.\")\n",
    " \n",
    "# Modell laden\n",
    "trained_model = Net(num_classes=43)\n",
    "trained_model.to(device)\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "trained_model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(trained_model, train_transforms_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(trained_model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Explainable Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Select a random or specific image by changing the index\n",
    "rand = 9  # Ändere die Zahl hier, um ein anderes Bild anzuzeigen\n",
    "image = images[rand:rand+1]\n",
    "label = labels[rand]\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "# Display the input image\n",
    "plt.imshow(image[0].permute(1, 2, 0).cpu())  # Bild vom Tensor in NumPy-Format umwandeln\n",
    "plt.title(f'Input Image - Label: {label.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the activations dictionary\n",
    "activations = {}\n",
    "# Ensure hooks are registered (they should be from earlier)\n",
    "# Run the model on the sample image\n",
    "output = trained_model(image)\n",
    "print(activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aktivierungen abrufen\n",
    "act = activations['conv_layer1']\n",
    "print(f\"Shape of conv1 activations: {act.shape}\")\n",
    "\n",
    "# Filter-Anzahl\n",
    "num_filters = act.shape[1]\n",
    "\n",
    "# Dynamische Subplots\n",
    "rows = math.ceil(num_filters / 5)\n",
    "fig, axes = plt.subplots(rows, 5, figsize=(15, rows * 3))\n",
    "\n",
    "for idx in range(num_filters):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(act[0, idx].detach().cpu(), cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "# Unbenutzte Subplots entfernen (falls nötig)\n",
    "for idx in range(num_filters, rows * 5):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Maximazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_maximization(model, layer_name, filter_index, input_size=(1, 3, 35, 35), lr=0.1, iterations=30):\n",
    "    device = next(model.parameters()).device  # Gerät des Modells (CPU oder GPU)\n",
    "    \n",
    "    # Initialisierung der Eingabe\n",
    "    input_image = torch.randn(input_size, requires_grad=True, device=device)  # Auf das richtige Gerät legen\n",
    "    \n",
    "    optimizer = optim.Adam([input_image], lr=lr, weight_decay=1e-6)\n",
    "    activations = {}\n",
    "\n",
    "    # Hook-Funktion für die gewünschte Schicht\n",
    "    def hook_function(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Hook registrieren\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_function)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)  # Modell auf dem richtigen Gerät ausführen\n",
    "        act = activations[layer_name][0, filter_index]\n",
    "        loss = -torch.mean(act)  # Aktivierung maximieren\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Eingabewerte beschränken\n",
    "        input_image.data = torch.clamp(input_image.data, 0, 1)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Iteration {i+1}/{iterations}, Loss: {-loss.item():.4f}\")\n",
    "\n",
    "    hook.remove()\n",
    "    return input_image.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer1num_filters_relu1 = model.conv_layer1.out_channels\n",
    "am_images_relu1 = []\n",
    "\n",
    "for filter_idx in range(conv_layer1num_filters_relu1):\n",
    "    print(f\"\\nGenerating image for relu1 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(model, 'relu1', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30)\n",
    "    am_images_relu1.append(am_image)\n",
    "\n",
    "print(\"\\nCompleted activation maximization for relu1.\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu1[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild darstellen\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu1 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu 2\n",
    "\n",
    "# Anzahl der Filter in der zweiten Schicht (ReLU2)\n",
    "conv_layer2num_filters_relu2 = model.conv_layer2.out_channels\n",
    "am_images_relu2 = []\n",
    "\n",
    "# Aktivierungsmaximierung für alle Filter der ReLU2-Schicht\n",
    "for filter_idx in range(conv_layer2num_filters_relu2):\n",
    "    print(f\"\\nGenerating image for relu2 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(\n",
    "        model, 'relu2', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30\n",
    "    )\n",
    "    am_images_relu2.append(am_image)\n",
    "\n",
    "print(\"\\nCompleted activation maximization for relu2.\")\n",
    "\n",
    "# Darstellung der Ergebnisse\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # Layout für 10 Filter\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu2[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu2 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dummy-Eingabebild erstellen (RGB-Bild, normalisiert zwischen 0 und 1) und auf das Gerät verschieben\n",
    "img, label = test_dataset[4000]  # Zufälliges Bild (Channels, Height, Width)\n",
    "input_image = img.to(device)  # Die Variable `img` sollte `input_image` zugewiesen werden.\n",
    "\n",
    "# Weiterleitung durch die Schichten bis ReLU2\n",
    "with torch.no_grad():\n",
    "    out_conv1 = model.relu1(model.conv_layer1(input_image))  # Erste Schicht + ReLU\n",
    "    out_pool1 = model.max_pool1(out_conv1)                  # Max-Pooling\n",
    "    out_conv2 = model.relu2(model.conv_layer2(out_pool1))   # Zweite Schicht + ReLU\n",
    "\n",
    "# Eingabebild visualisieren\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img.permute(1, 2, 0).cpu())  # (C, H, W) -> (H, W, C) und auf CPU für die Darstellung\n",
    "plt.axis('off')\n",
    "plt.title(\"Eingabebild\")\n",
    "plt.show()\n",
    "\n",
    "# Ausgabe-Shape der Aktivierung\n",
    "print(\"Shape der Aktivierung nach ReLU2:\", out_conv2.shape)\n",
    "\n",
    "# Aktivierungskarten der ersten 10 Filter visualisieren\n",
    "num_filters = min(10, out_conv2.shape[1])  # Zeige maximal 10 Filter\n",
    "fig, axes = plt.subplots(1, num_filters, figsize=(15, 6))\n",
    "\n",
    "for i in range(num_filters):\n",
    "    activation_map = out_conv2[0, i].cpu().numpy()  # Aktivierung für Filter i\n",
    "    \n",
    "    # Falls die Aktivierungskarten 1D sind, reshape sie zu 2D (H, W)\n",
    "    if activation_map.ndim == 1:\n",
    "        activation_map = activation_map.reshape(35, 35)  # Beispiel für die Größe 35x35 (oder was du benötigst)\n",
    "    \n",
    "    axes[i].imshow(activation_map, cmap='viridis')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle(\"Aktivierungskarten für ReLU2 (bestimmtes Bild)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def grad_cam(model, input_image, target_class, layer_name):\n",
    "    model.eval()  \n",
    "\n",
    "    # Dictionary to hold gradients and activations\n",
    "    gradients = {}\n",
    "    activations = {}\n",
    "\n",
    "    # Hook to capture gradients\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients[layer_name] = grad_output[0]\n",
    "\n",
    "    # Hook to capture activations\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Register hooks on the desired layer\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    input_image = input_image.unsqueeze(0)  # Add batch dimension\n",
    "    input_image = input_image.to(next(model.parameters()).device)  # Move to device\n",
    "    output = model(input_image)\n",
    "\n",
    "    # Backward pass for the target class\n",
    "    model.zero_grad()\n",
    "    target_score = output[0, target_class]\n",
    "    target_score.backward()\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    grads = gradients[layer_name]  # Gradients from backward pass\n",
    "    acts = activations[layer_name]  # Activations from forward pass\n",
    "    pooled_grads = torch.mean(grads, dim=(2, 3))  # Global average pooling\n",
    "\n",
    "    # Weight activations by pooled gradients\n",
    "    acts = acts * pooled_grads.view(1, -1, 1, 1)  # Apply broadcasting\n",
    "\n",
    "    # Average across the channels\n",
    "    heatmap = torch.mean(acts, dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "# Visualisierung der Grad-CAM\n",
    "def show_grad_cam(image, heatmap, alpha=0.5):\n",
    "    # Normalize image to [0, 1]\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    # Resize heatmap to match image dimensions using PyTorch\n",
    "    heatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    heatmap_resized = F.interpolate(heatmap_tensor, size=image.shape[:2], mode='bilinear', align_corners=False)\n",
    "    heatmap_resized = heatmap_resized.squeeze().numpy()  # Remove added dims\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)  # Scale heatmap to [0, 255]\n",
    "\n",
    "    # Convert heatmap to RGB\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]  # Convert heatmap to RGB\n",
    "    heatmap_colored = heatmap_colored / np.max(heatmap_colored)  # Normalize heatmap\n",
    "\n",
    "    # Combine heatmap with original image\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image\n",
    "    overlay = overlay / np.max(overlay)  # Normalize overlay to [0, 1]\n",
    "\n",
    "    # Plot the result\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Lade und transformiere ein Bild von einem Pfad\n",
    "def load_image(image_path, input_size=(3, 100, 100)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((input_size[1], input_size[2])),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = np.array(image) / 255.0  \n",
    "    image_tensor = transform(image)\n",
    "    return image_tensor, original_image\n",
    "\n",
    "\n",
    "image_path = \"C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/test_images/lkw_verboten/01374.ppm\"  # Beispielpfad\n",
    "image_tensor, original_image = load_image(image_path)\n",
    "\n",
    "# Zielklasse und Schicht\n",
    "layer_name = \"conv_layer5\" \n",
    "target_class = 0  \n",
    "\n",
    "heatmap = grad_cam(trained_model, image_tensor, target_class, layer_name)\n",
    "show_grad_cam(original_image, heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_image import LimeImageExplainer\n",
    "from skimage.segmentation import mark_boundaries, slic\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def transform_image_for_model(image, model_input_size=(3, 35, 35)):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((model_input_size[1], model_input_size[2])),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  \n",
    "\n",
    "def predict_proba(images):\n",
    "    \"\"\"\n",
    "    Wrapper-Funktion für LIME, um Modellvorhersagen für eine Reihe von Bildern zu generieren.\n",
    "    \"\"\"\n",
    "    images_tensor = torch.stack([torch.tensor(image).permute(2, 0, 1) for image in images]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(images_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "explainer = LimeImageExplainer()\n",
    "#Image path\n",
    "image_path = \"GTSRB/Final_Test/Images/100_kmh/00366.ppm\" \n",
    "example_image = Image.open(image_path)\n",
    "transformed_image = transform_image_for_model(example_image).squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "def custom_segmentation(image):\n",
    "    \"\"\"\n",
    "    Anpassen der Segmentierungsmethode für LIME.\n",
    "    \"\"\"\n",
    "    return slic(image, n_segments=50, compactness=30, start_label=1)\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    transformed_image,\n",
    "    predict_proba,\n",
    "    top_labels=1,\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    "    segmentation_fn=custom_segmentation \n",
    ")\n",
    "\n",
    "#Vis\n",
    "label_to_explain = explanation.top_labels[0]  \n",
    "temp, mask = explanation.get_image_and_mask(\n",
    "    label=label_to_explain,\n",
    "    positive_only=True,  \n",
    "    hide_rest=False,     \n",
    "    num_features=10,     \n",
    "    min_weight=0.01      \n",
    ")\n",
    "\n",
    "segments = slic(transformed_image, n_segments=50, compactness=30, start_label=1)\n",
    "boundaries_image = mark_boundaries(np.array(example_image) / 255.0, segments)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Originalbild\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(example_image)\n",
    "plt.title(\"Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Superpixel-Grenzen \n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(boundaries_image)\n",
    "plt.title(\"Superpixel-Grenzen über Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# LIME-\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mark_boundaries(temp / 255.0, mask))\n",
    "plt.title(f\"LIME-Visualisierung für Klasse {label_to_explain}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neuron auswählen\n",
    "target_neuron = 28  # Index des gewünschten Neurons (z. B. in der letzten Fully Connected Layer)\n",
    "print(train_dataset.classes[target_neuron])\n",
    "\n",
    "# Zufälliges Startbild erzeugen\n",
    "input_image = torch.randn(1, 3, 100, 100, requires_grad=True, device=device)  # Für 32x32 RGB-Bilder\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([input_image], lr=0.1)\n",
    "\n",
    "# Normalisierung (optional, falls dein Modell normalisierte Eingaben erwartet)\n",
    "normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "# Optimierung\n",
    "for step in range(200):  # Anzahl der Schritte für die Optimierung\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Vorwärtsdurchlauf\n",
    "    normalized_input = normalize(input_image)\n",
    "    outputs = model(normalized_input)\n",
    "\n",
    "    # Ziel: Maximierung der Aktivierung des gewählten Neurons\n",
    "    loss = -outputs[0, target_neuron]  # Negative Aktivierung (für Maximierung)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Regularisierung (z. B. Begrenzung der Pixelwerte)\n",
    "    with torch.no_grad():\n",
    "        input_image.clamp_(0, 1)  # Pixelwerte zwischen 0 und 1 halten\n",
    "\n",
    "    # Fortschritt anzeigen\n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualisierung des optimierten Bildes\n",
    "optimized_image = input_image.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "plt.imshow(optimized_image)\n",
    "plt.title(f\"Maximale Aktivierung von Neuron {target_neuron}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
