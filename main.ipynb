{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Jupyter Notebooks for implementing the German Traffic Sign Classification CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibt an, ob wir auf einer GPU oder CPU trainieren \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier wird unser mean und unser std für die Normalisierung der Bilder berechnet\n",
    "# dataset = ImageFolder(\"GTSRB\\Final_Training\\Images\", transform=transforms.ToTensor())\n",
    "transform_mean_std = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = ImageFolder(\"C:/Users/v814u63/Documents/Uni/5. Semester/XAI/images/train_images\", transform=transform_mean_std)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform definieren zum Einlesen der Test- und Trainingsdaten\n",
    "img_size = 35 \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist()) \n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(20),  # Zufällige Rotation um ±20 Grad\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),  # Zufälliges Zuschneiden und Skalieren\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Helligkeit und Farbvariation\n",
    "    transforms.RandomHorizontalFlip(),  # Zufälliges horizontales Spiegeln\n",
    "    transforms.ToTensor(),  # Umwandlung zu einem Tensor\n",
    "    transforms.Normalize(mean.tolist(), std.tolist())  # Normalisierung\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Beispiel-Transformation (ersetze dies mit deiner Transformation)\n",
    "transform = None  # Placeholder, hier eigene Transformation einfügen\n",
    "train_transforms = None  # Placeholder für Transformationen\n",
    "\n",
    "# Einlesen der Datasets\n",
    "train_dataset = ImageFolder(root='GTSRB/Final_Test/Images', transform=transform)\n",
    "test_dataset = ImageFolder(root='GTSRB/Final_Training/Images', transform=transform)\n",
    "\n",
    "# Erstellen einer Tabelle, die den Index und das zugehörige Label zeigt\n",
    "label_dict = {index: label for index, label in enumerate(train_dataset.classes)}\n",
    "df = pd.DataFrame(list(label_dict.items()), columns=['Index', 'Label'])\n",
    "print(df)\n",
    "\n",
    "# Bild und Label zur Überprüfung\n",
    "img, label = train_dataset[4000]\n",
    "label_string = train_dataset.classes[label]\n",
    "print(\"Label:\", label_string)\n",
    "\n",
    "# Konvertiere PIL-Image in Tensor, falls nötig\n",
    "if not isinstance(img, torch.Tensor):  # Prüfen, ob es ein PIL-Image ist\n",
    "    img = ToTensor()(img)\n",
    "\n",
    "# Bild anzeigen\n",
    "plt.imshow(img.permute(1, 2, 0))  # Tensor wird transformiert für die Darstellung\n",
    "plt.title(f\"Label: {label_string}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Dynamische Berechnung der FC-Eingabegröße\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(batch_size, 3, img_size, img_size)  # Dummy-Eingabe mit typischer Größe (z.B. 224x224)\n",
    "        fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(fc_input_size, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "        \"\"\"Hilfsfunktion, um die Eingabegröße für die Fully Connected Layers zu berechnen.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)  # Flatten the output\n",
    "        return x.size(1)  # Gib die Anzahl der Features zurück\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)  # Flatten\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderes CNN definieren zum Testen\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 750) \n",
    "        self.bn5 = nn.BatchNorm1d(750)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(750, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(self.bn3(x))\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the model and definition of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierung des Modells\n",
    "model = CNN(num_classes).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initiale Lernrate und Optimizer\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# ReduceLROnPlateau Scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(lr, weight_decay, threshold):\n",
    "    # Initialisierung des Modells\n",
    "    model = CNN(num_classes).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initiale Lernrate und Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ReduceLROnPlateau Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=threshold, verbose=True)\n",
    "    return model, loss_func, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'model/new_cnn_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with accuracy plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Activation of the Model Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function to capture the outputs\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN(num_classes=43)\n",
    "model.to(device)\n",
    "# Initiale Lernrate und Optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Register hooks to capture activations from convolutional and fully connected layers\n",
    "model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "\n",
    "# Register hooks for ReLU activations\n",
    "model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "\n",
    "# Register hooks for fully connected layers\n",
    "model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "\n",
    "# Register hooks for ReLU activations in fully connected layers\n",
    "model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "model.relu8.register_forward_hook(get_activation('relu8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Visualization while Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Zusätzliche Listen für neue Visualisierungen\n",
    "last_layer_weights = []\n",
    "pca_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "# Initialize the plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Variable für Loader-Wechsel\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None  # Speichert die Epoche des Loaderswechsels\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for the current epoch\n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_func(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Calculate average loss for the current epoch\n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "    # Accuracy on training and test data\n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Check if loss has increased and Loader hasn't switched yet\n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader  # Wechsel zu transformiertem Loader\n",
    "        switch_epoch = epoch  # Speichere die Epoche des Wechsels\n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    # Update the previous loss\n",
    "    previous_loss = avg_loss\n",
    "\n",
    "    # Visualisierung der Gewichte der letzten Schicht (vor dem Output Layer)\n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy()  # Anpassen, falls die Schicht anders heißt\n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    # PCA auf die Output-Vektoren anwenden\n",
    "    with torch.no_grad():\n",
    "        pca = PCA(n_components=2)\n",
    "        outputs_2d = pca.fit_transform(outputs.cpu().numpy())\n",
    "    pca_results.append(outputs_2d)\n",
    "\n",
    "    # Visualisierungen aktualisieren\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: PCA der Outputs\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('PCA of Output Vectors')\n",
    "    plt.xlabel('PCA Dimension 1')\n",
    "    plt.ylabel('PCA Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualisierung der letzten Layer-Gewichte\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "            print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "            plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "            break\n",
    "\n",
    "    # Display the updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# Save the model's weights\n",
    "torch.save(model.state_dict(), f'model/gute_modell.pth')\n",
    "torch.save(model.state_dict(), f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "# Show the final plot after all epochs\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Zusätzliche Listen für neue Visualisierungen\n",
    "last_layer_weights = []\n",
    "tsne_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "# Initialize the plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Variable für Loader-Wechsel\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None  # Speichert die Epoche des Loaderswechsels\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for the current epoch\n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_func(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Calculate average loss for the current epoch\n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "    # Accuracy on training and test data\n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Check if loss has increased and Loader hasn't switched yet\n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader  # Wechsel zu transformiertem Loader\n",
    "        switch_epoch = epoch  # Speichere die Epoche des Wechsels\n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    # Update the previous loss\n",
    "    previous_loss = avg_loss\n",
    "\n",
    "    # Visualisierung der Gewichte der letzten Schicht (vor dem Output Layer)\n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy()  # Anpassen, falls die Schicht anders heißt\n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    # t-SNE auf die Output-Vektoren anwenden\n",
    "    with torch.no_grad():\n",
    "        tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "        outputs_2d = tsne.fit_transform(outputs.cpu().numpy())\n",
    "    tsne_results.append(outputs_2d)\n",
    "\n",
    "    # Visualisierungen aktualisieren\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: t-SNE der Outputs\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('t-SNE of Output Vectors')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualisierung der letzten Layer-Gewichte\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "        print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "        plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "        break\n",
    "\n",
    "    # Display the updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# Save the model's weights\n",
    "torch.save(model.state_dict(), f'model/gute_modell.pth')\n",
    "torch.save(model.state_dict(), f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "# Show the final plot after all epochs\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code from yt \n",
    "(https://www.youtube.com/watch?v=ZBfpkepdZlw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\t\t# load in the data in batches\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # backward propagation and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # at end of epoch check validation loss and acc\n",
    "    with torch.no_grad():\n",
    "      \t# switch model to eval (not train) model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_loss = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total += labels.size(0)\n",
    "            # calculate predictions\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # calculate actual values\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # calculate the loss\n",
    "            all_val_loss.append(loss_func(outputs, labels).item())\n",
    "        # calculate val-loss\n",
    "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
    "        # calculate val-accuracy\n",
    "        mean_val_acc = 100 * (correct / total)\n",
    "    print(\n",
    "        'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(\n",
    "            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")\n",
    "\n",
    "#torch.save(model.state_dict(), \"model/good_models/99_95_and_95_77/training_99_95_test_95_77.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse, ohne die Anzahl der Klassen als Eingabe zu benötigen\n",
    "def calculate_class_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Sammle die Richtigkeit der Vorhersagen für jede Klasse\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    # Berechne die Accuracy für jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names\n",
    "\n",
    "# Berechne und plotte die Klassenaccuracy nach den 40 Trainingsepochen\n",
    "class_accuracies, class_names = calculate_class_accuracy(trained_model, test_loader, device)\n",
    "\n",
    "# Plot Histogramm der Klassenaccuracies mit Klassennamen auf der x-Achse\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(class_names)), class_accuracies, color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy per Class after Training')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)  # Setzt die Klassennamen als x-Achsen-Beschriftungen\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class and save wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Berechnet die Accuracy pro Klasse und speichert falsche Vorhersagen\n",
    "def calculate_class_accuracy_and_misclassifications(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Hole die Anzahl der Klassen und die Klassennamen aus dem Dataset\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    misclassified_counts = defaultdict(lambda: [0] * num_classes)  # Zählt Fehlklassifizierungen je Klasse\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Zählt korrekte und fehlerhafte Vorhersagen\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "                else:\n",
    "                    misclassified_counts[label][predicted[i].item()] += 1  # Fehlklassifizierung speichern\n",
    "    \n",
    "    # Berechne die Accuracy für jede Klasse\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names, misclassified_counts\n",
    "\n",
    "# Berechne Accuracy und Fehlklassifizierungen\n",
    "class_accuracies, class_names, misclassified_counts = calculate_class_accuracy_and_misclassifications(trained_model, test_loader, device)\n",
    "\n",
    "# Erstelle Plots für Klassen mit einer Accuracy < 80%\n",
    "for i, accuracy in enumerate(class_accuracies):\n",
    "    if accuracy < 80:\n",
    "        # Bereite Daten für die Fehlklassifizierungen dieser Klasse auf\n",
    "        misclassified_counts_for_class = misclassified_counts[i]\n",
    "        misclassified_class_names = [class_names[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        misclassified_class_counts = [misclassified_counts_for_class[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        \n",
    "        # Plot der Fehlklassifizierungen für die Klasse\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(misclassified_class_names, misclassified_class_counts, color='salmon')\n",
    "        plt.xlabel('Predicted Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f\"Misclassifications for Class '{class_names[i]}' (Accuracy: {accuracy:.2f}%)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to test an old model, you can do this right here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN Model Class vor Evaluation. Make sure you replace this with the correct Conv Layers and FC Layers of your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, image_size = 35):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Dynamische Berechnung der FC-Eingabegröße\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(batch_size, 3, image_size, image_size)  # Dummy-Eingabe mit typischer Größe (z.B. 224x224)\n",
    "        fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(fc_input_size, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "        \"\"\"Hilfsfunktion, um die Eingabegröße für die Fully Connected Layers zu berechnen.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)  # Flatten the output\n",
    "        return x.size(1)  # Gib die Anzahl der Features zurück\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)  # Flatten\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide now informations such as path and the number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-Pass abgeschlossen.\n",
      "Gespeicherte Aktivierungen: dict_keys(['conv_layer1', 'relu1', 'conv_layer2', 'relu2', 'conv_layer3', 'relu3', 'conv_layer4', 'relu4', 'conv_layer5', 'relu5', 'fc6', 'relu6', 'fc7', 'relu7', 'fc8', 'relu8', 'fc9'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v814u63\\AppData\\Local\\Temp\\ipykernel_12684\\3861788303.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Bildpfad angeben\n",
    "image_path = 'GTSRB\\Final_Test\\Images\\\\50_kmh\\\\00124.ppm'  # Hier den Pfad zum Bild angeben\n",
    "\n",
    "# Prüfen, ob der Modellpfad existiert\n",
    "model_path = 'model\\FINAL_Models\\SGD_FINAL_image_size_35_train_95.37351118365682_test_95.16920367341804.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Das Modell unter '{model_path}' wurde nicht gefunden.\")\n",
    "\n",
    "# Dictionary für die Aktivierungen\n",
    "activations = {}\n",
    "\n",
    "# Hook-Funktion, um die Ausgaben zu speichern\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialisierung des Modells\n",
    "trained_model = Net(num_classes=43)\n",
    "trained_model.to(device)\n",
    "\n",
    "# Modell laden\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Hooks für die Schichten des Modells registrieren\n",
    "trained_model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "trained_model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "trained_model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "trained_model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "trained_model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "\n",
    "# Hooks für die ReLU-Schichten registrieren\n",
    "trained_model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "trained_model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "trained_model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "trained_model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "trained_model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "\n",
    "# Hooks für Fully-Connected-Schichten registrieren\n",
    "trained_model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "trained_model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "trained_model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "trained_model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "\n",
    "# Hooks für ReLU in Fully-Connected-Schichten registrieren\n",
    "trained_model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "trained_model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "trained_model.relu8.register_forward_hook(get_activation('relu8'))\n",
    "\n",
    "# Bildvorverarbeitung definieren\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  # Größe anpassen\n",
    "    transforms.ToTensor(),       # In Tensor umwandeln\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalisierung\n",
    "])\n",
    "\n",
    "# Test Forward-Pass mit Bild\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Das Bild unter '{image_path}' wurde nicht gefunden.\")\n",
    "    \n",
    "    # Bild laden und vorverarbeiten\n",
    "    image = Image.open(image_path).convert('RGB')  # Bild in RGB umwandeln\n",
    "    image = transform(image).unsqueeze(0)         # Batch-Dimension hinzufügen\n",
    "    \n",
    "    # Bild durch das Modell leiten\n",
    "    image = image.to(device)\n",
    "    output = trained_model(image)\n",
    "    \n",
    "    print(\"Forward-Pass abgeschlossen.\")\n",
    "    print(\"Gespeicherte Aktivierungen:\", activations.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value in the output\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Determine the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "trained_model.to(device)\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = calculate_accuracy(trained_model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = calculate_accuracy(trained_model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Explainable Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Select a random or specific image by changing the index\n",
    "rand = 9  # Ändere die Zahl hier, um ein anderes Bild anzuzeigen\n",
    "image = images[rand:rand+1]\n",
    "label = labels[rand]\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "# Display the input image\n",
    "plt.imshow(image[0].permute(1, 2, 0).cpu())  # Bild vom Tensor in NumPy-Format umwandeln\n",
    "plt.title(f'Input Image - Label: {label.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabebild (Beispiel)\n",
    "image = torch.randn(1, 3, 35, 35).to(device)  # Simuliertes Bild (Batch-Größe 1)\n",
    "\n",
    "# Reset des Aktivierungs-Dictionaries\n",
    "activations = {}\n",
    "\n",
    "# Forward-Pass\n",
    "output = trained_model(image)\n",
    "\n",
    "# Gespeicherte Aktivierungen ausgeben\n",
    "print(\"Gespeicherte Aktivierungen:\", activations.keys())\n",
    "for layer, activation in activations.items():\n",
    "    print(f\"{layer}: {activation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aktivierungen abrufen\n",
    "act = activations['conv_layer1']\n",
    "print(f\"Shape of conv1 activations: {act.shape}\")\n",
    "\n",
    "# Filter-Anzahl\n",
    "num_filters = act.shape[1]\n",
    "\n",
    "# Dynamische Subplots\n",
    "rows = math.ceil(num_filters / 5)\n",
    "fig, axes = plt.subplots(rows, 5, figsize=(15, rows * 3))\n",
    "\n",
    "for idx in range(num_filters):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(act[0, idx].detach().cpu(), cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "# Unbenutzte Subplots entfernen (falls nötig)\n",
    "for idx in range(num_filters, rows * 5):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Maximazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_maximization(model, layer_name, filter_index, input_size=(1, 3, 35, 35), lr=0.1, iterations=30):\n",
    "    device = next(model.parameters()).device  # Gerät des Modells (CPU oder GPU)\n",
    "    \n",
    "    # Initialisierung der Eingabe\n",
    "    input_image = torch.randn(input_size, requires_grad=True, device=device)  # Auf das richtige Gerät legen\n",
    "    \n",
    "    optimizer = optim.SGD([input_image], lr=lr, weight_decay=1e-6)\n",
    "    activations = {}\n",
    "\n",
    "    # Hook-Funktion für die gewünschte Schicht\n",
    "    def hook_function(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Hook registrieren\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_function)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)  # Modell auf dem richtigen Gerät ausführen\n",
    "        act = activations[layer_name][0, filter_index]\n",
    "        loss = -torch.mean(act)  # Aktivierung maximieren\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Eingabewerte beschränken\n",
    "        input_image.data = torch.clamp(input_image.data, 0, 1)\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print(f\"Iteration {i+1}/{iterations}, Loss: {-loss.item():.4f}\")\n",
    "\n",
    "    hook.remove()\n",
    "    return input_image.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer1num_filters_relu1 = trained_model.conv_layer1.out_channels\n",
    "am_images_relu1 = []\n",
    "\n",
    "for filter_idx in range(conv_layer1num_filters_relu1):\n",
    "    #print(f\"\\nGenerating image for relu1 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(trained_model, 'relu1', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30)\n",
    "    am_images_relu1.append(am_image)\n",
    "\n",
    "#print(\"\\nCompleted activation maximization for relu1.\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu1[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild darstellen\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu1 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu 2\n",
    "\n",
    "# Anzahl der Filter in der zweiten Schicht (ReLU2)\n",
    "conv_layer2num_filters_relu2 = trained_model.conv_layer2.out_channels\n",
    "am_images_relu2 = []\n",
    "\n",
    "# Aktivierungsmaximierung für alle Filter der ReLU2-Schicht\n",
    "for filter_idx in range(conv_layer2num_filters_relu2):\n",
    "    #print(f\"\\nGenerating image for relu2 filter {filter_idx}\")\n",
    "    am_image = activation_maximization(\n",
    "        trained_model, 'relu2', filter_idx, input_size=(1, 3, 35, 35), lr=0.1, iterations=30\n",
    "    )\n",
    "    am_images_relu2.append(am_image)\n",
    "\n",
    "print(\"\\nCompleted activation maximization for relu2.\")\n",
    "\n",
    "# Darstellung der Ergebnisse\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # Layout für 10 Filter\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu2[:10]):  # Maximal 10 Bilder anzeigen\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy())  # RGB-Bild\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu2 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Modell und Gerät (z.B. CPU oder GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Bildvorverarbeitung definieren\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  # Größe anpassen, z.B. 35x35\n",
    "    transforms.ToTensor(),        # In Tensor umwandeln\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalisierung\n",
    "])\n",
    "\n",
    "# Bild laden und vorverarbeiten\n",
    "image = Image.open(image_path).convert('RGB')  # Bild in RGB umwandeln\n",
    "input_image = transform(image).unsqueeze(0)  # Batch-Dimension hinzufügen\n",
    "\n",
    "# Das Bild auf das Gerät verschieben\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Sicherstellen, dass das Bild eine Batch-Dimension hat\n",
    "# Die Variable `input_image` hat die Form (1, C, H, W)\n",
    "\n",
    "# Modell initialisieren und laden (stellen sicher, dass es auf dem richtigen Gerät ist)\n",
    "trained_model = Net(num_classes=43)  # Modell initialisieren\n",
    "trained_model.to(device)  # Auf das richtige Gerät verschieben\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Aktivierungen für den Forward-Pass bis zur ReLU2 berechnen\n",
    "with torch.no_grad():\n",
    "    out_conv1 = trained_model.relu1(trained_model.conv_layer1(input_image))  # Erste Schicht + ReLU\n",
    "    out_pool1 = trained_model.max_pool1(out_conv1)  # Max-Pooling\n",
    "    out_conv2 = trained_model.relu2(trained_model.conv_layer2(out_pool1))  # Zweite Schicht + ReLU\n",
    "\n",
    "# Eingabebild visualisieren\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image)  # Bild im Originalformat anzeigen\n",
    "plt.axis('off')\n",
    "plt.title(\"Eingabebild\")\n",
    "plt.show()\n",
    "\n",
    "# Ausgabe-Shape der Aktivierung\n",
    "print(\"Shape der Aktivierung nach ReLU1:\", out_conv1.shape)\n",
    "\n",
    "# Aktivierungskarten der ersten 10 Filter visualisieren\n",
    "num_filters = min(10, out_conv1.shape[1])  # Zeige maximal 10 Filter\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # Zwei Reihen mit je 5 Bildern\n",
    "\n",
    "for i in range(num_filters):\n",
    "    activation_map = out_conv1[0, i].cpu().numpy()  # Aktivierung für Filter i\n",
    "    \n",
    "    # Debugging: Form überprüfen\n",
    "    print(f\"Shape von Filter {i}: {activation_map.shape}\")\n",
    "    \n",
    "    # Falls die Aktivierungskarten 1D sind, versuche ein Reshape\n",
    "    if activation_map.ndim == 1:\n",
    "        size = int(activation_map.size ** 0.5)\n",
    "        assert size * size == activation_map.size, f\"Aktivierung {i} hat kein quadratisches Shape!\"\n",
    "        activation_map = activation_map.reshape(size, size)\n",
    "    \n",
    "    row = i // 5  # Bestimme die Zeile\n",
    "    col = i % 5   # Bestimme die Spalte\n",
    "    axes[row, col].imshow(activation_map, cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle(\"Featuremap für ReLU1 - Bad Model (42% Accuracy)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def grad_cam(model, input_image, target_class, layer_name):\n",
    "    model.eval()  \n",
    "\n",
    "    # Dictionary to hold gradients and activations\n",
    "    gradients = {}\n",
    "    activations = {}\n",
    "\n",
    "    # Hook to capture gradients\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients[layer_name] = grad_output[0]\n",
    "\n",
    "    # Hook to capture activations\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Register hooks on the desired layer\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    input_image = input_image.unsqueeze(0)  # Add batch dimension\n",
    "    input_image = input_image.to(next(model.parameters()).device)  # Move to device\n",
    "    output = model(input_image)\n",
    "\n",
    "    # Backward pass for the target class\n",
    "    model.zero_grad()\n",
    "    target_score = output[0, target_class]\n",
    "    target_score.backward()\n",
    "\n",
    "    # Compute Grad-CAM\n",
    "    grads = gradients[layer_name]  # Gradients from backward pass\n",
    "    acts = activations[layer_name]  # Activations from forward pass\n",
    "    pooled_grads = torch.mean(grads, dim=(2, 3))  # Global average pooling\n",
    "\n",
    "    # Weight activations by pooled gradients\n",
    "    acts = acts * pooled_grads.view(1, -1, 1, 1)  # Apply broadcasting\n",
    "\n",
    "    # Average across the channels\n",
    "    heatmap = torch.mean(acts, dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "# Visualisierung der Grad-CAM\n",
    "def show_grad_cam(image, heatmap, alpha=0.5):\n",
    "    # Normalize image to [0, 1]\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    # Resize heatmap to match image dimensions using PyTorch\n",
    "    heatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    heatmap_resized = F.interpolate(heatmap_tensor, size=image.shape[:2], mode='bilinear', align_corners=False)\n",
    "    heatmap_resized = heatmap_resized.squeeze().numpy()  # Remove added dims\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)  # Scale heatmap to [0, 255]\n",
    "\n",
    "    # Convert heatmap to RGB\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]  # Convert heatmap to RGB\n",
    "    heatmap_colored = heatmap_colored / np.max(heatmap_colored)  # Normalize heatmap\n",
    "\n",
    "    # Combine heatmap with original image\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image\n",
    "    overlay = overlay / np.max(overlay)  # Normalize overlay to [0, 1]\n",
    "\n",
    "    # Plot the result\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Lade und transformiere ein Bild von einem Pfad\n",
    "def load_image(image_path, input_size=(3, 35, 35)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((input_size[1], input_size[2])),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = np.array(image) / 255.0  \n",
    "    image_tensor = transform(image)\n",
    "    return image_tensor, original_image\n",
    "\n",
    "\n",
    "image_tensor, original_image = load_image(image_path)\n",
    "\n",
    "# Zielklasse und Schicht\n",
    "layer_name = \"conv_layer5\" \n",
    "target_class = 4\n",
    "\n",
    "heatmap = grad_cam(trained_model, image_tensor, target_class, layer_name)\n",
    "show_grad_cam(original_image, heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_image import LimeImageExplainer\n",
    "from skimage.segmentation import mark_boundaries, slic\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def transform_image_for_model(image):\n",
    "    \"\"\"\n",
    "    Transformiert das Bild zur Modell-Eingabegröße.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((70, 70)),  # Bild bleibt bei 70x70 für LIME\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def predict_proba(images):\n",
    "    \"\"\"\n",
    "    Wrapper-Funktion für LIME, um Modellvorhersagen für eine Reihe von Bildern zu generieren.\n",
    "    \"\"\"\n",
    "    # Transformiert Bilder zurück auf 35x35 für das Modell\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((35, 35)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.fromarray((image * 255).astype(np.uint8))).to(device) for image in images\n",
    "    ])\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(images_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialisierung des LIME-Explainers\n",
    "explainer = LimeImageExplainer()\n",
    "# Bildpfad\n",
    "\n",
    "# Laden und Anpassen des Bildes auf 70x70 für LIME\n",
    "example_image = Image.open(image_path).convert('RGB')\n",
    "transformed_image = transform_image_for_model(example_image)\n",
    "transformed_image_np = np.array(transformed_image) / 255.0  # Normalisierung auf [0, 1]\n",
    "\n",
    "def custom_segmentation(image):\n",
    "    \"\"\"\n",
    "    Angepasste Segmentierungsmethode für LIME.\n",
    "    \"\"\"\n",
    "    return slic(image, n_segments=50, compactness=10, start_label=1)\n",
    "\n",
    "# Generierung der LIME-Erklärung\n",
    "explanation = explainer.explain_instance(\n",
    "    transformed_image_np,\n",
    "    predict_proba,\n",
    "    top_labels=1,\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    "    segmentation_fn=custom_segmentation\n",
    ")\n",
    "# Visualisierung\n",
    "label_to_explain = explanation.top_labels[0]\n",
    "temp, mask = explanation.get_image_and_mask(\n",
    "    label=label_to_explain,\n",
    "    positive_only=True,\n",
    "    hide_rest=False,\n",
    "    num_features=10,\n",
    "    min_weight=0.01\n",
    ")\n",
    "\n",
    "segments = slic(transformed_image_np, n_segments=50, compactness=10, start_label=1)\n",
    "boundaries_image = mark_boundaries(transformed_image_np, segments)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Originalbild\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(example_image.resize((70, 70)))\n",
    "plt.title(\"Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Superpixel-Grenzen über Originalbild\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(boundaries_image)\n",
    "plt.title(\"Superpixel-Grenzen über Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "# LIME-Visualisierung\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.title(f\"LIME-Visualisierung für Klasse {label_to_explain} - {test_dataset.classes[label_to_explain]}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letzte Schicht, Neuron 19, Iteration 0/100, Verlust: -0.9251832365989685\n",
      "Letzte Schicht, Neuron 19, Iteration 10/100, Verlust: -14.83403205871582\n",
      "Letzte Schicht, Neuron 19, Iteration 20/100, Verlust: -26.234554290771484\n",
      "Letzte Schicht, Neuron 19, Iteration 30/100, Verlust: -33.97446060180664\n",
      "Letzte Schicht, Neuron 19, Iteration 40/100, Verlust: -39.299774169921875\n",
      "Letzte Schicht, Neuron 19, Iteration 50/100, Verlust: -43.02858352661133\n",
      "Letzte Schicht, Neuron 19, Iteration 60/100, Verlust: -45.81037139892578\n",
      "Letzte Schicht, Neuron 19, Iteration 70/100, Verlust: -48.114871978759766\n",
      "Letzte Schicht, Neuron 19, Iteration 80/100, Verlust: -49.91350173950195\n",
      "Letzte Schicht, Neuron 19, Iteration 90/100, Verlust: -51.376468658447266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Zielneuron in der letzten Schicht definieren\n",
    "target_neuron = 19  # Index des Neurons in der letzten Schicht\n",
    "\n",
    "# Zufälliges Startbild erzeugen\n",
    "input_size = (1, 3, 35, 35)  # Eingabegröße an dein Modell anpassen\n",
    "input_image = torch.randn(input_size, requires_grad=True, device='cuda')  # Zufällige Eingabe (GPU genutzt)\n",
    "\n",
    "# Ordner erstellen, um die Bilder zu speichern\n",
    "os.makedirs('max_neuron_images', exist_ok=True)\n",
    "\n",
    "# Modell auf Evaluation setzen\n",
    "trained_model.eval()\n",
    "\n",
    "# Optimierer definieren\n",
    "optimizer = optim.SGD([input_image], lr=0.1)\n",
    "\n",
    "# Optimierungsschleife\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward Pass\n",
    "    output = trained_model(input_image)\n",
    "\n",
    "    # Aktivierung für das Zielneuron extrahieren (letzte Schicht)\n",
    "    neuron_activation = output[0, target_neuron]\n",
    "\n",
    "    # Verlust als negative Aktivierung definieren (wird maximiert)\n",
    "    loss = -neuron_activation\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimierungsschritt\n",
    "    optimizer.step()\n",
    "\n",
    "    # Eingabebildwerte beschränken\n",
    "    input_image.data.clamp_(0, 1)\n",
    "\n",
    "    # Fortschritt anzeigen\n",
    "    if i % 10 == 0:\n",
    "        print(f'Letzte Schicht, Neuron {target_neuron}, Iteration {i}/{num_iterations}, Verlust: {loss.item()}')\n",
    "\n",
    "# Ergebnis visualisieren\n",
    "result = input_image.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "# Normalisieren für die Darstellung\n",
    "result = (result - result.min()) / (result.max() - result.min())\n",
    "\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "plt.title(f'Letzte Schicht, Neuron {target_neuron}')\n",
    "plt.show()\n",
    "\n",
    "# Bild speichern\n",
    "plt.imsave(f'max_neuron_images/letzte_schicht_neuron_{target_neuron}.png', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Stellen Sie sicher, dass Ihr Modell im Evaluationsmodus ist\n",
    "trained_model.eval()\n",
    "\n",
    "# Gerät konfigurieren (CPU oder GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model.to(device)\n",
    "\n",
    "# Transformationen definieren, die auf das Bild angewendet werden\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  # Passen Sie die Größe an die des Modells an\n",
    "    transforms.ToTensor(),        # Konvertieren Sie das Bild in einen Tensor\n",
    "    # Optional: Normalisierung hinzufügen, falls Ihr Modell dies erfordert\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Optional: Liste der Klassennamen definieren\n",
    "class_names = [\n",
    "    'Geschwindigkeitsbegrenzung (20km/h)',\n",
    "    'Geschwindigkeitsbegrenzung (30km/h)',\n",
    "    'Geschwindigkeitsbegrenzung (50km/h)',\n",
    "    # ... fügen Sie alle 43 Klassennamen hinzu\n",
    "    'Andere Gefahren'\n",
    "]\n",
    "\n",
    "def predict_image_class(image_path):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild, transformiert es und gibt die Wahrscheinlichkeiten der Top-3-Klassen zurück.\n",
    "    \"\"\"\n",
    "    # Bild laden\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Bild transformieren\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Batch-Dimension hinzufügen\n",
    "    \n",
    "    # Vorhersage durchführen\n",
    "    with torch.no_grad():\n",
    "        output = trained_model(image_tensor)\n",
    "        # Wahrscheinlichkeiten berechnen\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        # Top-3-Klassen und -Wahrscheinlichkeiten erhalten\n",
    "        top_probs, top_indices = torch.topk(probabilities, 3)\n",
    "        top_probs = top_probs.cpu().numpy()[0]\n",
    "        top_indices = top_indices.cpu().numpy()[0]\n",
    "    \n",
    "    # Ergebnisse als Liste von Tupeln (Klassenindex, Wahrscheinlichkeit) zurückgeben\n",
    "    top_classes = [(int(idx), float(prob)) for idx, prob in zip(top_indices, top_probs)]\n",
    "    return top_classes\n",
    "\n",
    "# Beispielverwendung\n",
    "image_path = 'output.png'  # Geben Sie den Pfad zu Ihrem Bild an\n",
    "top_classes = predict_image_class(image_path)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print('Die Top-3-Klassen sind:')\n",
    "for idx, prob in top_classes:\n",
    "    if idx < len(class_names):\n",
    "        class_name = class_names[idx]\n",
    "    else:\n",
    "        class_name = f'Klasse {idx}'\n",
    "    print(f'{class_name}: {prob * 100:.2f}% Wahrscheinlichkeit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def visualize_fc_layer_with_path_and_top_neurons(model, layer_name, image_path, top_n=5):\n",
    "    model.eval()\n",
    "    activations = {}\n",
    "\n",
    "    # Hook für das gewünschte Layer\n",
    "    def hook_fn(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Layer abrufen und Hook setzen\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Bild laden und transformieren\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((35, 35)),  \n",
    "        transforms.ToTensor(),      \n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_image = transform(image).unsqueeze(0).to(next(model.parameters()).device)  # Auf Batch-Dimension erweitern\n",
    "\n",
    "    # Bild durch das Modell schicken\n",
    "    with torch.no_grad():\n",
    "        model(input_image)\n",
    "\n",
    "    features = activations[layer_name][0].cpu().detach().numpy()\n",
    "\n",
    "    top_indices = np.argsort(features)[-top_n:][::-1]  \n",
    "    top_activations = features[top_indices]  \n",
    "\n",
    "    print(f\"Die {top_n} stärksten aktivierten Neuronen in {layer_name}:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. Neuron {idx}: Aktivierung = {top_activations[i]:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['red' if i in top_indices else 'blue' for i in range(len(features))]\n",
    "    plt.bar(range(len(features)), features, color=colors)\n",
    "    plt.title(f'Aktivierungen aus {layer_name} (rot = Top-{top_n} Neuronen)')\n",
    "    plt.xlabel('Neuron Index')\n",
    "    plt.ylabel('Aktivierung')\n",
    "\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        plt.text(idx, features[idx], f\"{idx}\", ha='center', va='bottom', fontsize=9, color='red')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "visualize_fc_layer_with_path_and_top_neurons(trained_model, 'fc9', image_path, top_n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
