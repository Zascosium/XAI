{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Jupyter Notebooks for implementing the German Traffic Sign Classification CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import numpy as np \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# LIME and segmentation for explanations\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from skimage.segmentation import mark_boundaries, slic\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the selected device to ensure clarity for the user\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and std calculation for the normalization of the images\n",
    "transform_mean_std = transforms.Compose([\n",
    "    transforms.Resize((35, 35)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = ImageFolder(\"GTSRB/Final_Training/Images\", transform=transform_mean_std)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform for importing test and training data\n",
    "img_size = 35 \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist()) \n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),  \n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data and subsequent output of an image incl. label for checking\n",
    "train_dataset = ImageFolder(root='GTSRB/Final_Training/Images', transform=transform)\n",
    "train_transforms_dataset = ImageFolder(root='GTSRB/Final_Training/Images', transform=train_transforms)\n",
    "\n",
    "test_dataset = ImageFolder(root='GTSRB/Final_Test/Images', transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True)\n",
    "num_classes = 43\n",
    "\n",
    "# --------- Testing ---------\n",
    "img, label = train_transforms_dataset[2092]\n",
    "label_string = test_dataset.classes[label]\n",
    "print(\"Label:\", label_string)\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model that was used for training\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Since we have also trained with different image sizes, we have written a helper function that calculates what kind of vector with how many dimensions comes out of the conv layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(batch_size, 3, img_size, img_size)\n",
    "        fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(fc_input_size, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "        # Helper function to calculate the input size for the Fully Connected Layers\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)\n",
    "        return x.size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)  \n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have tested a different model in two places here. However, this was not pursued any further\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 750) \n",
    "        self.bn5 = nn.BatchNorm1d(750)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(750, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(self.bn3(x))\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the model and definition of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "model = CNN(num_classes).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initial learning rate and optimizer\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# At times we have also used a ReduceLROnPlateau Scheduler. However, this approach was not pursued further on.\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the model and definition of hyperparameters and get the Activation of the Model Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "# Function to create hooks that capture activations\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Initialize the model and move it to the selected device (CPU or GPU)\n",
    "model = CNN(num_classes=43).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Register hooks for each convolutional layer and its ReLU activation\n",
    "model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "\n",
    "model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "\n",
    "# Register hooks for each fully connected layer and its ReLU activation\n",
    "model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "\n",
    "model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "model.relu8.register_forward_hook(get_activation('relu8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function is defined here to calculate the accuracy of the model during and after training\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the default training, without any plots.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), 'model/new_cnn_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization while Training (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we train our model with different plots. The training is basically the same, but Accuracy, Loss, PCA and the activation of the last layer are also plotted\n",
    "last_layer_weights = []\n",
    "pca_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# The loader is also defined here. During the training, we realized that we can partially solve our problem with overfitting through data augmentation. We have therefore structured the training in such a way that we train with the normal training data as long as the loss decreases. As soon as the loss does not decrease from one epoch to the next, the loader is switched to the data with data augmentation and 10 further epochs are trained\n",
    "\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = loss_func(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    \n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "   \n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader \n",
    "        switch_epoch = epoch \n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    \n",
    "    previous_loss = avg_loss\n",
    "\n",
    "   \n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy()  \n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pca = PCA(n_components=2)\n",
    "        outputs_2d = pca.fit_transform(outputs.cpu().numpy())\n",
    "    pca_results.append(outputs_2d)\n",
    "\n",
    "    \n",
    "    plt.clf()  \n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: PCA of Output Vectors\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('PCA of Output Vectors')\n",
    "    plt.xlabel('PCA Dimension 1')\n",
    "    plt.ylabel('PCA Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualization of the last layer weights\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "            print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "            plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "            break\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f'model/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same is done here as in the code in the cell above. The difference is that TSNE is used here to visualize the output vectors\n",
    "last_layer_weights = []\n",
    "tsne_results = []\n",
    "\n",
    "num_epochs = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "current_loader = train_loader\n",
    "previous_loss = float('inf')\n",
    "switch_epoch = None \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for i, (images, labels) in enumerate(current_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = loss_func(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    \n",
    "    avg_loss = running_loss / len(current_loader)\n",
    "\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(model, current_loader, device)\n",
    "    test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "   \n",
    "    if avg_loss > previous_loss and current_loader == train_loader:\n",
    "        print(\"Loss has increased. Switching to train_transforms_loader.\")\n",
    "        current_loader = train_transforms_loader  \n",
    "        switch_epoch = epoch \n",
    "        print(\"Training will continue for exactly 10 epochs after loader switch.\")\n",
    "\n",
    "    \n",
    "    previous_loss = avg_loss\n",
    "\n",
    "    \n",
    "    last_fc_weights = model.fc9.weight.data.cpu().numpy() \n",
    "    last_layer_weights.append(last_fc_weights)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "        outputs_2d = tsne.fit_transform(outputs.cpu().numpy())\n",
    "    tsne_results.append(outputs_2d)\n",
    "\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: t-SNE of Output Vectors \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Class Index')\n",
    "    plt.title('t-SNE of Output Vectors')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    # Plot 4: Visualization of the last layer weights\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(last_fc_weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Weight Value')\n",
    "    plt.title('Weights of Last FC Layer')\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Classes')\n",
    "\n",
    "    if switch_epoch is not None and epoch >= switch_epoch + 10:\n",
    "        print(f\"Stopping training after 10 epochs post-switch at epoch {epoch + 1}.\")\n",
    "        plt.savefig(f'model/optim_loop/RMSprop/train_{train_accuracy}_test_{test_accuracy}.png')\n",
    "        break\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f'model/train_{train_accuracy}_test_{test_accuracy}.pth')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current model can be tested here\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "print(f\"Accuracy on training data: {train_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Accuracy on test data: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can calculate how accurate the model is for the individual classes\n",
    "def calculate_class_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names\n",
    "\n",
    "class_accuracies, class_names = calculate_class_accuracy(model, test_loader, device)\n",
    "\n",
    "# Plot the Accuracy per Class\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(class_names)), class_accuracies, color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy per Class after Training')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy per Class, save wrong predictions as number and then plot what was predicted instead of the correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy per Class, save wrong predictions as number and then plot what was predicted instead of the correct prediction\n",
    "def calculate_class_accuracy_and_misclassifications(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    class_names = data_loader.dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    misclassified_counts = defaultdict(lambda: [0] * num_classes)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    class_correct[label] += 1\n",
    "                else:\n",
    "                    misclassified_counts[label][predicted[i].item()] += 1  \n",
    "    \n",
    "    # Calculate class accuracy\n",
    "    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
    "    return class_accuracy, class_names, misclassified_counts\n",
    "\n",
    "\n",
    "# Calculate class accuracy and misclassifications\n",
    "class_accuracies, class_names, misclassified_counts = calculate_class_accuracy_and_misclassifications(model, test_loader, device)\n",
    "\n",
    "# Plot misclassifications for classes with accuracy below 80%\n",
    "for i, accuracy in enumerate(class_accuracies):\n",
    "    if accuracy < 80:\n",
    "        misclassified_counts_for_class = misclassified_counts[i]\n",
    "        misclassified_class_names = [class_names[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        misclassified_class_counts = [misclassified_counts_for_class[j] for j in range(len(misclassified_counts_for_class)) if misclassified_counts_for_class[j] > 0]\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(misclassified_class_names, misclassified_class_counts, color='salmon')\n",
    "        plt.xlabel('Predicted Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f\"Misclassifications for Class '{class_names[i]}' (Accuracy: {accuracy:.2f}%)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to test an old model, you can do this right here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN Model Class vor Evaluation. Make sure you replace this with the correct Conv Layers and FC Layers of your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a different model, possibly even with a different architecture, is to be tested, the new architecture can be defined here\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, img_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=4, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "       \n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(batch_size, 3, img_size, img_size)  \n",
    "        fc_input_size = self._get_fc_input_size(dummy_input)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc6 = nn.Linear(fc_input_size, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(256, 128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.fc9 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_fc_input_size(self, dummy_input):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.conv_layer1(dummy_input)\n",
    "            x = self.relu1(x)\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.max_pool2(x)\n",
    "\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.relu3(x)\n",
    "\n",
    "            x = self.conv_layer4(x)\n",
    "            x = self.relu4(x)\n",
    "\n",
    "            x = self.conv_layer5(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.max_pool5(x)\n",
    "\n",
    "            x = self.flatten(x)\n",
    "        return x.size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.dropout8(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "\n",
    "        out = self.fc9(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide now informations such as path and the number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path and model path\n",
    "image_path = 'GTSRB/Final_Test/Images/fussgaenger/01699.ppm' \n",
    "model_path = 'model/optim_loop/SGD/image_size_70_train_96.02897293988624_test_97.19461567492766.pth'\n",
    "img_size = 70\n",
    "# Check for existence\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Das Modell unter '{model_path}' wurde nicht gefunden.\")\n",
    "if not os.path.exists(image_path):\n",
    "    raise FileNotFoundError(f\"Das Bild unter '{image_path}' wurde nicht gefunden.\")\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_classes=43, img_size = img_size)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks for the layers\n",
    "model.conv_layer1.register_forward_hook(get_activation('conv_layer1'))\n",
    "model.conv_layer2.register_forward_hook(get_activation('conv_layer2'))\n",
    "model.conv_layer3.register_forward_hook(get_activation('conv_layer3'))\n",
    "model.conv_layer4.register_forward_hook(get_activation('conv_layer4'))\n",
    "model.conv_layer5.register_forward_hook(get_activation('conv_layer5'))\n",
    "model.relu1.register_forward_hook(get_activation('relu1'))\n",
    "model.relu2.register_forward_hook(get_activation('relu2'))\n",
    "model.relu3.register_forward_hook(get_activation('relu3'))\n",
    "model.relu4.register_forward_hook(get_activation('relu4'))\n",
    "model.relu5.register_forward_hook(get_activation('relu5'))\n",
    "model.fc6.register_forward_hook(get_activation('fc6'))\n",
    "model.fc7.register_forward_hook(get_activation('fc7'))\n",
    "model.fc8.register_forward_hook(get_activation('fc8'))\n",
    "model.fc9.register_forward_hook(get_activation('fc9'))\n",
    "model.relu6.register_forward_hook(get_activation('relu6'))\n",
    "model.relu7.register_forward_hook(get_activation('relu7'))\n",
    "model.relu8.register_forward_hook(get_activation('relu8'))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  # Ensure resizing to 70x70\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "image = image.to(device)\n",
    "output = model(image)\n",
    "\n",
    "print(\"Forward-Pass abgeschlossen.\")\n",
    "print(\"Gespeicherte Aktivierungen:\", activations.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Explainable Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation of the individual filters of the conv layer for the image defined when loading the model can be displayed here\n",
    "act = activations['conv_layer1']\n",
    "print(f\"Shape of conv1 activations: {act.shape}\")\n",
    "\n",
    "num_filters = act.shape[1]\n",
    "\n",
    "# Calculate number of rows for the plot\n",
    "rows = math.ceil(num_filters / 5)\n",
    "\n",
    "# Create subplots to display the filters\n",
    "fig, axes = plt.subplots(rows, 5, figsize=(15, rows * 3))\n",
    "\n",
    "# Display each filter activation\n",
    "for idx in range(num_filters):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(act[0, idx].detach().cpu(), cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "for idx in range(num_filters, rows * 5):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Maximazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function optimizes an image to maximize the activation of a specific filter in a CNN layer.\n",
    "def activation_maximization(model, layer_name, filter_index, input_size=(1, 3, img_size, img_size), lr=0.1, iterations=30):\n",
    "    device = next(model.parameters()).device  \n",
    "    \n",
    "    input_image = torch.randn(input_size, requires_grad=True, device=device)  \n",
    "    \n",
    "    # CHANGE THE OPTIMIZER TO THE CORRECT ONE! \n",
    "    optimizer = optim.SGD([input_image], lr=lr, weight_decay=1e-6)  \n",
    "    activations = {}  \n",
    "\n",
    "    # Hook function to store activations from the layer\n",
    "    def hook_function(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    # Register the hook for the given layer\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_function)\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)\n",
    "        act = activations[layer_name][0, filter_index]\n",
    "        loss = -torch.mean(act)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        input_image.data = torch.clamp(input_image.data, 0, 1)\n",
    "\n",
    "    hook.remove()  \n",
    "    return input_image.detach()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Maximization for relu1, first 10 Filters will be shown \n",
    "conv_layer1num_filters_relu1 = model.conv_layer1.out_channels\n",
    "am_images_relu1 = []\n",
    "\n",
    "for filter_idx in range(conv_layer1num_filters_relu1):\n",
    "    am_image = activation_maximization(model, 'relu1', filter_idx, input_size=(1, 3, img_size, img_size), lr=0.1, iterations=30)\n",
    "    am_images_relu1.append(am_image)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for idx, am_image in enumerate(am_images_relu1[:10]): \n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(am_image.squeeze().permute(1, 2, 0).cpu().numpy()) \n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {idx}')\n",
    "\n",
    "plt.suptitle('Activation Maximization Images for relu1 Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes the original image and displays the activation maps (feature maps) of the first 10 filters after the first ReLU layer.\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "input_image = transform(image).unsqueeze(0)  \n",
    "\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "model = Net(num_classes=43)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_conv1 = model.relu1(model.conv_layer1(input_image))  \n",
    "    out_pool1 = model.max_pool1(out_conv1)\n",
    "    out_conv2 = model.relu2(model.conv_layer2(out_pool1))\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Eingabebild\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Shape der Aktivierung nach ReLU1:\", out_conv1.shape)\n",
    "\n",
    "num_filters = min(10, out_conv1.shape[1])  # Limit to first 10 filters\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Visualize the activation maps of the first filters\n",
    "for i in range(num_filters):\n",
    "    activation_map = out_conv1[0, i].cpu().numpy()\n",
    "\n",
    "    print(f\"Shape von Filter {i}: {activation_map.shape}\")\n",
    "\n",
    "    if activation_map.ndim == 1:\n",
    "        size = int(activation_map.size ** 0.5)\n",
    "        assert size * size == activation_map.size, f\"Aktivierung {i} hat kein quadratisches Shape!\"\n",
    "        activation_map = activation_map.reshape(size, size)\n",
    "    \n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    axes[row, col].imshow(activation_map, cmap='viridis')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle(\"Featuremap für ReLU1 - 70x70\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Grad Cam visualization is implemented here\n",
    "def grad_cam(model, input_image, target_class, layer_name):\n",
    "    model.eval()  \n",
    "\n",
    "    gradients = {}\n",
    "    activations = {}\n",
    "\n",
    "    # Hooks to capture activations and gradients\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients[layer_name] = grad_output[0]\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    input_image = input_image.unsqueeze(0).to(next(model.parameters()).device)  \n",
    "    output = model(input_image)\n",
    "\n",
    "    model.zero_grad()\n",
    "    target_score = output[0, target_class] \n",
    "    target_score.backward()\n",
    "\n",
    "    grads = gradients[layer_name]\n",
    "    acts = activations[layer_name]\n",
    "    pooled_grads = torch.mean(grads, dim=(2, 3)) \n",
    "\n",
    "    acts = acts * pooled_grads.view(1, -1, 1, 1)\n",
    "    heatmap = torch.mean(acts, dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)  \n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def show_grad_cam(image, heatmap, alpha=0.5):\n",
    "    image = (image - image.min()) / (image.max() - image.min())  \n",
    "\n",
    "    heatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)\n",
    "    heatmap_resized = F.interpolate(heatmap_tensor, size=image.shape[:2], mode='bilinear', align_corners=False)\n",
    "    heatmap_resized = heatmap_resized.squeeze().numpy() \n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized) \n",
    "\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]  \n",
    "    heatmap_colored = heatmap_colored / np.max(heatmap_colored)\n",
    "\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image  \n",
    "    overlay = overlay / np.max(overlay)\n",
    "\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_image(image_path, input_size=(3, 100, 100)):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = np.array(image) / 255.0  \n",
    "    image_tensor = transform(image)\n",
    "    return image_tensor, original_image\n",
    "\n",
    "\n",
    "image_tensor, original_image = load_image(image_path)\n",
    "\n",
    "# The desired layer and the \"target class\" must be defined here.\n",
    "layer_name = \"conv_layer4\" \n",
    "target_class = 41\n",
    "\n",
    "heatmap = grad_cam(model, image_tensor, target_class, layer_name)  # Generate GradCAM heatmap\n",
    "show_grad_cam(original_image, heatmap)  # Display heatmap overlay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for resizing image to 70x70 for model\n",
    "def transform_image_for_model(image):\n",
    "    transform_70x70 = transforms.Compose([\n",
    "        transforms.Resize((70, 70)), \n",
    "    ])\n",
    "    return transform_70x70(image)\n",
    "\n",
    "# Predict the probabilities for each class\n",
    "def predict_proba(images):\n",
    "    # Convert the images to tensor format\n",
    "    images_tensor = torch.stack([\n",
    "        transform(Image.fromarray((image * 255).astype(np.uint8))).to(device) for image in images\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeImageExplainer()\n",
    "example_image = Image.open(image_path).convert('RGB')\n",
    "transformed_image = transform_image_for_model(example_image)\n",
    "transformed_image_np = np.array(transformed_image) / 255.0\n",
    "\n",
    "# Custom segmentation function for LIME\n",
    "def custom_segmentation(image):\n",
    "    return slic(image, n_segments=50, compactness=10, start_label=1)\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    transformed_image_np,\n",
    "    predict_proba,\n",
    "    top_labels=1,\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    "    segmentation_fn=custom_segmentation\n",
    ")\n",
    "\n",
    "label_to_explain = explanation.top_labels[0]\n",
    "temp, mask = explanation.get_image_and_mask(\n",
    "    label=label_to_explain,\n",
    "    positive_only=True,\n",
    "    hide_rest=False,\n",
    "    num_features=10,\n",
    "    min_weight=0.01\n",
    ")\n",
    "\n",
    "# Generate superpixels and boundaries\n",
    "segments = slic(transformed_image_np, n_segments=50, compactness=10, start_label=1)\n",
    "boundaries_image = mark_boundaries(transformed_image_np, segments)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(example_image.resize((70, 70)))\n",
    "plt.title(\"Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot Superpixel Boundaries\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(boundaries_image)\n",
    "plt.title(\"Superpixel-Grenzen über Originalbild\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot LIME Explanation\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.title(f\"LIME-Visualisierung für Klasse {label_to_explain} - {test_dataset.classes[label_to_explain]}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Final plot adjustments\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here a neuron can be maximally activated to see and output the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the 43 output neurons\n",
    "target_neuron = 19\n",
    "\n",
    "# Set input size and initialize random image\n",
    "input_size = (1, 3, img_size, img_size)  \n",
    "input_image = torch.randn(input_size, requires_grad=True, device=device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.RMSprop([input_image], lr=0.1)\n",
    "for neuron in range(num_classes):\n",
    "    num_iterations = 100\n",
    "    for i in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_image)\n",
    "        neuron_activation = output[0, neuron]\n",
    "        loss = -neuron_activation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        input_image.data.clamp_(0, 1)\n",
    "\n",
    "    # Process and display the resulting image\n",
    "    result = input_image.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    result = (result - result.min()) / (result.max() - result.min())\n",
    "\n",
    "    plt.imshow(result)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Neuron {neuron} - {test_dataset.classes[neuron]}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows which 3 classes the imported model considers most probable\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def predict_image_class(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probabilities, 3)\n",
    "        top_probs = top_probs.cpu().numpy()[0]\n",
    "        top_indices = top_indices.cpu().numpy()[0]\n",
    "    \n",
    "    top_classes = [(int(idx), float(prob)) for idx, prob in zip(top_indices, top_probs)]\n",
    "    return top_classes\n",
    "\n",
    "# Get and print top-3 predicted classes\n",
    "top_classes = predict_image_class(image_path)\n",
    "\n",
    "print('Die Top-3-Klassen sind:')\n",
    "for idx, prob in top_classes:\n",
    "    if idx < len(class_names):\n",
    "        class_name = class_names[idx]\n",
    "    else:\n",
    "        class_name = f'Klasse {idx}'\n",
    "    print(f'{class_name}: {prob * 100:.2f}% Wahrscheinlichkeit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map of last FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation, i.e. the feature map of the last Fully Connected Layer, can be output here\n",
    "def visualize_fc_layer_with_path_and_top_neurons(model, layer_name, image_path, top_n=5):\n",
    "    model.eval()\n",
    "    activations = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_image = transform(image).unsqueeze(0).to(next(model.parameters()).device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_image)\n",
    "\n",
    "    # Extract activations and get top N neurons\n",
    "    features = activations[layer_name][0].cpu().detach().numpy()\n",
    "    top_indices = np.argsort(features)[-top_n:][::-1]  \n",
    "    top_activations = features[top_indices]  \n",
    "\n",
    "    print(f\"Die {top_n} stärksten aktivierten Neuronen in {layer_name}:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. Neuron {idx}: Aktivierung = {top_activations[i]:.4f}\")\n",
    "\n",
    "    # Plot activation values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['red' if i in top_indices else 'blue' for i in range(len(features))]\n",
    "    plt.bar(range(len(features)), features, color=colors)\n",
    "    plt.title(f'Aktivierungen aus {layer_name} (rot = Top-{top_n} Neuronen)')\n",
    "    plt.xlabel('Neuron Index')\n",
    "    plt.ylabel('Aktivierung')\n",
    "\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        plt.text(idx, features[idx], f\"{idx}\", ha='center', va='bottom', fontsize=9, color='red')\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "visualize_fc_layer_with_path_and_top_neurons(model, 'fc9', image_path, top_n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
